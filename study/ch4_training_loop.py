"""
=============================================================================
 Chapter 4: Training Loop (í•™ìŠµ ë£¨í”„)
 - microgpt.py L145~183ì— í•´ë‹¹í•˜ëŠ” ì½”ë“œ
 - explain.mdì˜ Training loop, Adam optimizer ì„¹ì…˜ ë‚´ìš©ì„ í•œêµ­ì–´ ì£¼ì„ìœ¼ë¡œ í¬í•¨
 - "emma" ë‹¨ì–´ í•˜ë‚˜ë¡œ 5ìŠ¤í…ë§Œ í•™ìŠµ (ì˜¤ë²„í”¼íŒ… ì‹¤í—˜)
 - ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥: python study/ch4_training_loop.py
=============================================================================
"""

import os
import math
import random
random.seed(42)

# ë””ë²„ê±°(LazyVim ë“±)ì—ì„œ cwdê°€ study/ì¼ ë•Œ input.txtë¥¼ ì°¾ê¸° ìœ„í•´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
os.chdir(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì „ ì¤€ë¹„: Value í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Value:
    __slots__ = ('data', 'grad', '_children', '_local_grads')

    def __init__(self, data, children=(), local_grads=()):
        self.data = data
        self.grad = 0
        self._children = children
        self._local_grads = local_grads

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data + other.data, (self, other), (1, 1))

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data * other.data, (self, other), (other.data, self.data))

    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))
    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))
    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))
    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))
    def __neg__(self): return self * -1
    def __radd__(self, other): return self + other
    def __sub__(self, other): return self + (-other)
    def __rsub__(self, other): return other + (-self)
    def __rmul__(self, other): return self * other
    def __truediv__(self, other): return self * other**-1
    def __rtruediv__(self, other): return other * self**-1

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        self.grad = 1
        for v in reversed(topo):
            for child, local_grad in zip(v._children, v._local_grads):
                child.grad += local_grad * v.grad

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì „ ì¤€ë¹„: í† í¬ë‚˜ì´ì €
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not os.path.exists('input.txt'):
    import urllib.request
    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt'
    urllib.request.urlretrieve(names_url, 'input.txt')
docs = [line.strip() for line in open('input.txt') if line.strip()]
random.shuffle(docs)

uchars = sorted(set(''.join(docs)))
BOS = len(uchars)
vocab_size = len(uchars) + 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì „ ì¤€ë¹„: ëª¨ë¸ (ì¶•ì†Œ íŒŒë¼ë¯¸í„°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
n_layer = 1
n_embd = 4        # ì¶•ì†Œ: ì›ë³¸ 16 â†’ 4
block_size = 8    # ì¶•ì†Œ: ì›ë³¸ 16 â†’ 8
n_head = 2        # ì¶•ì†Œ: ì›ë³¸ 4 â†’ 2
head_dim = n_embd // n_head

matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]
state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}
for i in range(n_layer):
    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)
    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)
params = [p for mat in state_dict.values() for row in mat for p in row]

def linear(x, w):
    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]

def softmax(logits):
    max_val = max(val.data for val in logits)
    exps = [(val - max_val).exp() for val in logits]
    total = sum(exps)
    return [e / total for e in exps]

def rmsnorm(x):
    ms = sum(xi * xi for xi in x) / len(x)
    scale = (ms + 1e-5) ** -0.5
    return [xi * scale for xi in x]

def gpt(token_id, pos_id, keys, values):
    tok_emb = state_dict['wte'][token_id]
    pos_emb = state_dict['wpe'][pos_id]
    x = [t + p for t, p in zip(tok_emb, pos_emb)]
    x = rmsnorm(x)
    for li in range(n_layer):
        x_residual = x
        x = rmsnorm(x)
        q = linear(x, state_dict[f'layer{li}.attn_wq'])
        k = linear(x, state_dict[f'layer{li}.attn_wk'])
        v = linear(x, state_dict[f'layer{li}.attn_wv'])
        keys[li].append(k)
        values[li].append(v)
        x_attn = []
        for h in range(n_head):
            hs = h * head_dim
            q_h = q[hs:hs+head_dim]
            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]
            v_h = [vi[hs:hs+head_dim] for vi in values[li]]
            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]
            attn_weights = softmax(attn_logits)
            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]
            x_attn.extend(head_out)
        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])
        x = [a + b for a, b in zip(x, x_residual)]
        x_residual = x
        x = rmsnorm(x)
        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])
        x = [xi.relu() for xi in x]
        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])
        x = [a + b for a, b in zip(x, x_residual)]
    logits = linear(x, state_dict['lm_head'])
    return logits

print(f"vocab_size: {vocab_size}, params: {len(params)}")


# =============================================================================
# ì„¹ì…˜ 1: í•™ìŠµ ë£¨í”„ í•´ë¶€ (microgpt.py L145~183)
# =============================================================================
#
# [explain.md - Training loop ì„¹ì…˜ ìš”ì•½]
# í•™ìŠµ ë£¨í”„ëŠ” ë°˜ë³µì ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•œë‹¤:
#   (1) ë¬¸ì„œë¥¼ ì„ íƒí•œë‹¤
#   (2) ëª¨ë¸ì„ ìˆœì „íŒŒ(forward)ì‹œí‚¨ë‹¤
#   (3) ì†ì‹¤(loss)ì„ ê³„ì‚°í•œë‹¤
#   (4) ì—­ì „íŒŒ(backward)ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ êµ¬í•œë‹¤
#   (5) íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤
#
# â”€â”€â”€â”€â”€â”€â”€â”€ í•™ìŠµ ë£¨í”„ì˜ ê° ë‹¨ê³„ â”€â”€â”€â”€â”€â”€â”€â”€
#
# í† í°í™” (Tokenization):
#   ê° í•™ìŠµ ìŠ¤í…ì€ í•˜ë‚˜ì˜ ë¬¸ì„œë¥¼ ì„ íƒí•˜ê³  ì–‘ìª½ì— BOSë¥¼ ë¶™ì¸ë‹¤:
#   "emma" â†’ [BOS, e, m, m, a, BOS]
#   ëª¨ë¸ì˜ ì„ë¬´: ì´ì „ í† í°ë“¤ì´ ì£¼ì–´ì¡Œì„ ë•Œ ê°ê°ì˜ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ.
#
# ìˆœì „íŒŒì™€ ì†ì‹¤ (Forward pass and loss):
#   í† í°ë“¤ì„ í•˜ë‚˜ì”© ëª¨ë¸ì— ë„£ì–´ KV ìºì‹œë¥¼ ìŒ“ì•„ê°„ë‹¤.
#   ê° ìœ„ì¹˜ì—ì„œ ëª¨ë¸ì€ 27ê°œ ë¡œì§“ ì¶œë ¥ â†’ softmaxë¡œ í™•ë¥  ë³€í™˜.
#   ì†ì‹¤ = ì •ë‹µ í† í°ì˜ ìŒì˜ ë¡œê·¸ í™•ë¥ : -log(p(target))
#   ì´ê²ƒì´ cross-entropy loss.
#   ì§ê´€: ëª¨ë¸ì´ ì‹¤ì œ ë‹¤ìŒ í† í°ì— ì–¼ë§ˆë‚˜ "ë†€ëëŠ”ê°€"
#   - ì •ë‹µì— í™•ë¥  1.0 â†’ ì†ì‹¤ 0 (ì „í˜€ ë†€ë¼ì§€ ì•ŠìŒ)
#   - ì •ë‹µì— í™•ë¥  ~0   â†’ ì†ì‹¤ â†’ +âˆ (ë§¤ìš° ë†€ëŒ)
#
# ì—­ì „íŒŒ (Backward pass):
#   loss.backward() í•œ ë²ˆ í˜¸ì¶œë¡œ ì „ì²´ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ì—­ì „íŒŒ.
#   ê° íŒŒë¼ë¯¸í„°ì˜ .gradì— "ì´ íŒŒë¼ë¯¸í„°ë¥¼ ì–´ë–»ê²Œ ë°”ê¾¸ë©´ lossê°€ ì¤„ì–´ë“œëŠ”ì§€" ì €ì¥.
#
# [explain.md - Adam optimizer ì„¹ì…˜ ìš”ì•½]
# Adam ì˜µí‹°ë§ˆì´ì €:
#   ë‹¨ìˆœíˆ p.data -= lr * p.grad (ê²½ì‚¬ í•˜ê°•ë²•)ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ,
#   Adamì€ ë” ë˜‘ë˜‘í•˜ë‹¤:
#   - m: ìµœê·¼ ê·¸ë˜ë””ì–¸íŠ¸ì˜ í‰ê·  (ëª¨ë©˜í…€, êµ´ëŸ¬ê°€ëŠ” ê³µì²˜ëŸ¼)
#   - v: ìµœê·¼ ê·¸ë˜ë””ì–¸íŠ¸ ì œê³±ì˜ í‰ê·  (íŒŒë¼ë¯¸í„°ë³„ í•™ìŠµë¥  ì¡°ì ˆ)
#   - m_hat, v_hat: í¸í–¥ ë³´ì • (m, vê°€ 0ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ ì›Œë°ì—… í•„ìš”)
#   - í•™ìŠµë¥ ì€ í•™ìŠµ ì¤‘ ì„ í˜•ìœ¼ë¡œ ê°ì†Œ
#   - ì—…ë°ì´íŠ¸ í›„ .grad = 0ìœ¼ë¡œ ë¦¬ì…‹ (ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n" + "=" * 60)
print("ğŸ‹ï¸ [í•™ìŠµ ì‹¤í—˜] 'emma' í•˜ë‚˜ë¡œ 5ìŠ¤í… í•™ìŠµ")
print("=" * 60)

# --- ì›ë³¸ ì½”ë“œ (microgpt.py L145~148) ---
# Adam ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8
m_buf = [0.0] * len(params)  # 1ì°¨ ëª¨ë©˜í…€ ë²„í¼ (ê·¸ë˜ë””ì–¸íŠ¸ í‰ê· )
v_buf = [0.0] * len(params)  # 2ì°¨ ëª¨ë©˜í…€ ë²„í¼ (ê·¸ë˜ë””ì–¸íŠ¸ ì œê³± í‰ê· )

# í•™ìŠµí•  ë‹¨ì–´
doc = "emma"
tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]
n = len(tokens) - 1  # ì˜ˆì¸¡í•  ìœ„ì¹˜ ìˆ˜ = 5, block_sizeë³´ë‹¤ ì‘ìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ

num_steps = 5
print(f"\n  í•™ìŠµ ë‹¨ì–´: '{doc}'")
print(f"  í† í°ì—´: {tokens} (= [BOS, 'e', 'm', 'm', 'a', BOS])")
print(f"  ì˜ˆì¸¡ ìœ„ì¹˜ ìˆ˜: {n}")
print(f"  í•™ìŠµ ìŠ¤í… ìˆ˜: {num_steps}")

# í•™ìŠµ ì „ íŒŒë¼ë¯¸í„° ëª‡ ê°œ ìŠ¤ëƒ…ìƒ· (ë¹„êµìš©)
sample_params_idx = [0, 1, 2]
print(f"\n  [í•™ìŠµ ì „] íŒŒë¼ë¯¸í„° ìƒ˜í”Œ (wte[0][:3]):")
for i in sample_params_idx:
    print(f"    params[{i}]: data={params[i].data:+.6f}")

print(f"\n{'â”€' * 60}")

# =============================================================================
# í•™ìŠµ ë£¨í”„ (microgpt.py L152~183)
# =============================================================================
for step in range(num_steps):
    print(f"\n  â•â•â•â•â•â• STEP {step + 1}/{num_steps} â•â•â•â•â•â•")

    # â”€â”€â”€ (1) Forward pass: í† í°ì„ ëª¨ë¸ì— ë„£ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]
    losses = []

    print(f"\n  [Forward Pass]")
    for pos_id in range(n):
        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]
        logits = gpt(token_id, pos_id, keys, values)
        probs = softmax(logits)
        loss_t = -probs[target_id].log()
        losses.append(loss_t)

        # â”€â”€â”€ ë””ë²„ê·¸ í¬ì¸íŠ¸: ê° ìœ„ì¹˜ì˜ ì˜ˆì¸¡ ê´€ì°° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        input_str = "[BOS]" if token_id == BOS else f"'{uchars[token_id]}'"
        target_str = "[BOS]" if target_id == BOS else f"'{uchars[target_id]}'"
        target_prob = probs[target_id].data

        # Top 3 ì˜ˆì¸¡
        prob_pairs = sorted([(i, p.data) for i, p in enumerate(probs)], key=lambda x: -x[1])[:3]
        top3_str = ", ".join(
            f"{'[BOS]' if idx == BOS else uchars[idx]}:{p:.3f}" for idx, p in prob_pairs
        )

        print(f"    pos {pos_id}: {input_str:>5s} â†’ ì •ë‹µ={target_str:>5s}  "
              f"P(ì •ë‹µ)={target_prob:.4f}  loss={loss_t.data:.4f}  "
              f"top3=[{top3_str}]")

    # â”€â”€â”€ (2) í‰ê·  ì†ì‹¤ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    loss = (1 / n) * sum(losses)
    print(f"\n  [Loss] í‰ê·  ì†ì‹¤ = {loss.data:.4f}")

    # â”€â”€â”€ (3) Backward pass â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    loss.backward()

    # â”€â”€â”€ ë””ë²„ê·¸ í¬ì¸íŠ¸: backward í›„ ê·¸ë˜ë””ì–¸íŠ¸ ê´€ì°° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n  [Backward í›„ â€” ê·¸ë˜ë””ì–¸íŠ¸ ìƒ˜í”Œ]")
    # wte ì„ë² ë”©ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê´€ì°°
    print(f"    wte[BOS={BOS}] (í•™ìŠµì— ì‚¬ìš©ëœ ì„ë² ë”©):")
    bos_emb = state_dict['wte'][BOS]
    for j in range(min(n_embd, 4)):
        print(f"      wte[{BOS}][{j}]: data={bos_emb[j].data:+.6f}, grad={bos_emb[j].grad:+.6f}")

    # ì‚¬ìš©ë˜ì§€ ì•Šì€ í† í°ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë„ í™•ì¸
    unused_token = 25  # 'z' â€” "emma"ì— ì—†ëŠ” ë¬¸ì
    unused_emb = state_dict['wte'][unused_token]
    unused_grad = sum(abs(unused_emb[j].grad) for j in range(n_embd))
    print(f"    wte['z'={unused_token}] grad í•© = {unused_grad:.6f}  â† í•™ìŠµì— ì•ˆ ì“°ì—¬ì„œ 0ì— ê°€ê¹Œì›€!")

    # â”€â”€â”€ (4) Adam ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #
    # [explain.md ìš”ì•½]
    # m[i] = Î²â‚ Â· m[i] + (1-Î²â‚) Â· grad         â† ëª¨ë©˜í…€ (ê·¸ë˜ë””ì–¸íŠ¸ ì´ë™ í‰ê· )
    # v[i] = Î²â‚‚ Â· v[i] + (1-Î²â‚‚) Â· gradÂ²        â† ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ì¶”ì 
    # m_hat = m[i] / (1 - Î²â‚^(t+1))             â† í¸í–¥ ë³´ì •
    # v_hat = v[i] / (1 - Î²â‚‚^(t+1))             â† í¸í–¥ ë³´ì •
    # p.data -= lr_t Â· m_hat / (âˆšv_hat + Îµ)      â† íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
    #
    # ì§ê´€: "ê·¸ë˜ë””ì–¸íŠ¸ê°€ ê¾¸ì¤€íˆ ê°™ì€ ë°©í–¥ì´ë©´ ë” í¬ê²Œ ì›€ì§ì´ê³  (m),
    #       ê·¸ë˜ë””ì–¸íŠ¸ê°€ í° íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµë¥ ì„ ì¤„ì¸ë‹¤ (v)"

    lr_t = learning_rate * (1 - step / num_steps)  # ì„ í˜• í•™ìŠµë¥  ê°ì†Œ

    # ì—…ë°ì´íŠ¸ ì „í›„ ë¹„êµë¥¼ ìœ„í•´ ìŠ¤ëƒ…ìƒ·
    before_data = [params[i].data for i in sample_params_idx]

    for i, p in enumerate(params):
        m_buf[i] = beta1 * m_buf[i] + (1 - beta1) * p.grad
        v_buf[i] = beta2 * v_buf[i] + (1 - beta2) * p.grad ** 2
        m_hat = m_buf[i] / (1 - beta1 ** (step + 1))
        v_hat = v_buf[i] / (1 - beta2 ** (step + 1))
        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)
        p.grad = 0  # ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´ ê·¸ë˜ë””ì–¸íŠ¸ ë¦¬ì…‹!

    # â”€â”€â”€ ë””ë²„ê·¸ í¬ì¸íŠ¸: Adam ì—…ë°ì´íŠ¸ ì „í›„ ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n  [Adam ì—…ë°ì´íŠ¸] lr_t = {lr_t:.4f}")
    print(f"    íŒŒë¼ë¯¸í„° ë³€í™” ìƒ˜í”Œ (wte[0][:3]):")
    for idx, i in enumerate(sample_params_idx):
        delta = params[i].data - before_data[idx]
        print(f"      params[{i}]: {before_data[idx]:+.6f} â†’ {params[i].data:+.6f}  (Î”={delta:+.6f})")

print(f"\n{'â”€' * 60}")


# =============================================================================
# í•™ìŠµ ê²°ê³¼ ê´€ì°°: lossê°€ ì¤„ì–´ë“¤ì—ˆëŠ”ê°€?
# =============================================================================

print("\n" + "=" * 60)
print("ğŸ“‰ [ê²°ê³¼] í•™ìŠµ í›„ â€” ê°™ì€ ë¬¸ì„œë¡œ ë‹¤ì‹œ forward")
print("=" * 60)

# í•™ìŠµ í›„ ê°™ì€ ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ë„£ì–´ë³´ê¸°
keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]
losses = []
print(f"\n  '{doc}' ì¬í‰ê°€:")
for pos_id in range(n):
    token_id, target_id = tokens[pos_id], tokens[pos_id + 1]
    logits = gpt(token_id, pos_id, keys, values)
    probs = softmax(logits)
    loss_t = -probs[target_id].log()
    losses.append(loss_t)

    input_str = "[BOS]" if token_id == BOS else f"'{uchars[token_id]}'"
    target_str = "[BOS]" if target_id == BOS else f"'{uchars[target_id]}'"
    target_prob = probs[target_id].data

    prob_pairs = sorted([(i, p.data) for i, p in enumerate(probs)], key=lambda x: -x[1])[:3]
    top3_str = ", ".join(
        f"{'[BOS]' if idx == BOS else uchars[idx]}:{p:.3f}" for idx, p in prob_pairs
    )

    print(f"    pos {pos_id}: {input_str:>5s} â†’ ì •ë‹µ={target_str:>5s}  "
          f"P(ì •ë‹µ)={target_prob:.4f}  loss={loss_t.data:.4f}  "
          f"top3=[{top3_str}]")

final_loss = (1 / n) * sum(l.data for l in losses)
print(f"\n  ìµœì¢… í‰ê·  ì†ì‹¤ = {final_loss:.4f}")
print(f"  â†’ í•™ìŠµ ì „ loss (~3.3) ëŒ€ë¹„ ì¤„ì–´ë“¤ì—ˆëŠ”ê°€? {'âœ… Yes!' if final_loss < 3.3 else 'ì•„ì§ ë¶€ì¡±'}")


# =============================================================================
# í•µì‹¬ ì •ë¦¬
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ¯ í•µì‹¬ ì •ë¦¬")
print("=" * 60)
print(f"""
  í•™ìŠµ ë£¨í”„ì˜ í•µì‹¬ ì‚¬ì´í´:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  1. Forward: í† í° ì…ë ¥ â†’ ëª¨ë¸ â†’ ë¡œì§“ â†’ í™•ë¥   â”‚
  â”‚  2. Loss: -log(ì •ë‹µ í™•ë¥ ) = cross-entropy    â”‚
  â”‚  3. Backward: loss.backward() â†’ ëª¨ë“  grad    â”‚
  â”‚  4. Adam: grad ê¸°ë°˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸         â”‚
  â”‚  5. Reset: grad = 0, ë‹¤ìŒ ìŠ¤í… ì¤€ë¹„          â”‚
  â”‚  â†’ 1ë¡œ ëŒì•„ê°€ì„œ ë°˜ë³µ!                        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  í•µì‹¬ ê´€ì°°:
  - lossê°€ ìŠ¤í…ë§ˆë‹¤ (ëŒ€ì²´ë¡œ) ì¤„ì–´ë“ ë‹¤ â†’ ëª¨ë¸ì´ í•™ìŠµí•˜ê³  ìˆë‹¤!
  - P(ì •ë‹µ)ì´ ì ì  ë†’ì•„ì§„ë‹¤ â†’ ì˜ˆì¸¡ì´ ì ì  ì •í™•í•´ì§„ë‹¤
  - ì‚¬ìš©ë˜ì§€ ì•Šì€ í† í°('z')ì˜ grad â‰ˆ 0 â†’ ê´€ë ¨ ì—†ëŠ” íŒŒë¼ë¯¸í„°ëŠ” ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤
  - Adamì˜ ëª¨ë©˜í…€(m)ê³¼ ì ì‘ì  í•™ìŠµë¥ (v)ì´ ë‹¨ìˆœ SGDë³´ë‹¤ íš¨ê³¼ì 

  ğŸ’¡ ì´ ì‹¤í—˜ì€ "emma" í•˜ë‚˜ì— ì˜¤ë²„í”¼íŒ…í•˜ëŠ” ê²ƒ.
     ì‹¤ì œ í•™ìŠµì—ì„œëŠ” 32,000ê°œ ì´ë¦„ì„ ìˆœí™˜í•˜ë©° ì¼ë°˜í™”ë¥¼ í•™ìŠµí•œë‹¤.
     ë‹¤ìŒ ë‹¨ê³„: num_stepsë¥¼ ëŠ˜ë¦¬ê±°ë‚˜, ì—¬ëŸ¬ ë¬¸ì„œë¡œ í•™ìŠµí•´ë³´ê¸°!
""")
