"""
=============================================================================
 Chapter 2: Autograd (ìë™ ë¯¸ë¶„)
 - microgpt.py L29~71ì— í•´ë‹¹í•˜ëŠ” Value í´ë˜ìŠ¤ ì½”ë“œ ê·¸ëŒ€ë¡œ
 - explain.mdì˜ Autograd ì„¹ì…˜ ë‚´ìš©ì„ í•œêµ­ì–´ ì£¼ì„ìœ¼ë¡œ í¬í•¨
 - ì‹¤í—˜ ìˆœì„œ: ì‚¬ì¹™ì—°ì‚° â†’ ë¶„ê¸° ê·¸ë˜í”„ â†’ softmax+cross-entropy
 - ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥: python study/ch2_autograd.py
=============================================================================
"""

import math  # math.log, math.exp

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¹ì…˜ 1: Value í´ë˜ìŠ¤ (microgpt.py L29~71 ì›ë³¸ ê·¸ëŒ€ë¡œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# [explain.md - Autograd ì„¹ì…˜ ìš”ì•½]
# ì‹ ê²½ë§ í•™ìŠµì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸(gradient)ê°€ í•„ìš”í•˜ë‹¤:
# "ëª¨ë¸ì˜ ê° íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ê¸ˆ ì›€ì§ì´ë©´, ì†ì‹¤(loss)ì´ ì˜¬ë¼ê°€ëŠ”ê°€ ë‚´ë ¤ê°€ëŠ”ê°€, ì–¼ë§ˆë‚˜?"
#
# ê³„ì‚° ê·¸ë˜í”„(computation graph)ëŠ” ë§ì€ ì…ë ¥(ëª¨ë¸ íŒŒë¼ë¯¸í„° + ì…ë ¥ í† í°)ì„ ë°›ì•„
# í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ ì¶œë ¥ì¸ ì†ì‹¤(loss)ë¡œ ìˆ˜ë ´í•œë‹¤.
# ì—­ì „íŒŒ(backpropagation)ëŠ” ê·¸ ì¶œë ¥ì—ì„œ ì‹œì‘í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ê±°ê¾¸ë¡œ ë”°ë¼ê°€ë©°
# ëª¨ë“  ì…ë ¥ì— ëŒ€í•œ ì†ì‹¤ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•œë‹¤.
# ì´ê²ƒì€ ë¯¸ì ë¶„í•™ì˜ ì—°ì‡„ ë²•ì¹™(chain rule)ì— ì˜ì¡´í•œë‹¤.
#
# Value í´ë˜ìŠ¤ëŠ” í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ ìˆ«ì(.data)ë¥¼ ê°ì‹¸ê³ ,
# ê·¸ê²ƒì´ ì–´ë–»ê²Œ ê³„ì‚°ë˜ì—ˆëŠ”ì§€ë¥¼ ì¶”ì í•œë‹¤.
# ê° ì—°ì‚°ì€ ë ˆê³  ë¸”ë¡ ê°™ë‹¤:
#   - ì…ë ¥ì„ ë°›ê³ , ì¶œë ¥ì„ ë§Œë“ ë‹¤ (ìˆœì „íŒŒ, forward pass)
#   - ì¶œë ¥ì´ ê° ì…ë ¥ì— ëŒ€í•´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì•ˆë‹¤ (ë¡œì»¬ ê·¸ë˜ë””ì–¸íŠ¸, local gradient)
# ë‚˜ë¨¸ì§€ëŠ” ì—°ì‡„ ë²•ì¹™(chain rule)ìœ¼ë¡œ ë¸”ë¡ë“¤ì„ ì—°ê²°í•˜ëŠ” ê²ƒë¿ì´ë‹¤.
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚  ì—°ì‚° ë ˆê³  ë¸”ë¡ í…Œì´ë¸”                                        â”‚
# â”‚  ì—°ì‚°        ìˆœì „íŒŒ         ë¡œì»¬ ê·¸ë˜ë””ì–¸íŠ¸                    â”‚
# â”‚  a + b      a + b         âˆ‚/âˆ‚a = 1, âˆ‚/âˆ‚b = 1               â”‚
# â”‚  a * b      a Â· b         âˆ‚/âˆ‚a = b, âˆ‚/âˆ‚b = a               â”‚
# â”‚  a ** n     a^n           âˆ‚/âˆ‚a = nÂ·a^(n-1)                  â”‚
# â”‚  log(a)     ln(a)         âˆ‚/âˆ‚a = 1/a                        â”‚
# â”‚  exp(a)     e^a           âˆ‚/âˆ‚a = e^a                        â”‚
# â”‚  relu(a)    max(0, a)     âˆ‚/âˆ‚a = 1 if a > 0 else 0         â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Value:
    __slots__ = ('data', 'grad', '_children', '_local_grads')  # ë©”ëª¨ë¦¬ ìµœì í™”

    def __init__(self, data, children=(), local_grads=()):
        self.data = data                # ìˆœì „íŒŒì—ì„œ ê³„ì‚°ëœ ì´ ë…¸ë“œì˜ ìŠ¤ì¹¼ë¼ ê°’
        self.grad = 0                   # ì´ ë…¸ë“œì— ëŒ€í•œ ì†ì‹¤ì˜ ë¯¸ë¶„ê°’ (ì—­ì „íŒŒì—ì„œ ê³„ì‚°)
        self._children = children       # ê³„ì‚° ê·¸ë˜í”„ì—ì„œ ì´ ë…¸ë“œì˜ ìì‹ë“¤
        self._local_grads = local_grads # ìì‹ë“¤ì— ëŒ€í•œ ì´ ë…¸ë“œì˜ ë¡œì»¬ ë¯¸ë¶„ê°’

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data + other.data, (self, other), (1, 1))

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data * other.data, (self, other), (other.data, self.data))

    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))
    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))
    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))
    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))
    def __neg__(self): return self * -1
    def __radd__(self, other): return self + other
    def __sub__(self, other): return self + (-other)
    def __rsub__(self, other): return other + (-self)
    def __rmul__(self, other): return self * other
    def __truediv__(self, other): return self * other**-1
    def __rtruediv__(self, other): return other * self**-1

    def backward(self):
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # [explain.md í•µì‹¬ ì„¤ëª…]
        # backward()ëŠ” ê·¸ë˜í”„ë¥¼ ì—­ìœ„ìƒì •ë ¬(reverse topological order)ë¡œ ìˆœíšŒí•œë‹¤.
        # (ì†ì‹¤ì—ì„œ ì‹œì‘í•´ì„œ íŒŒë¼ë¯¸í„°ì—ì„œ ëë‚¨)
        #
        # ë…¸ë“œ vê°€ ìì‹ cë¥¼ ê°€ì§€ê³ , ë¡œì»¬ ê·¸ë˜ë””ì–¸íŠ¸ê°€ âˆ‚v/âˆ‚cë¼ë©´:
        #   âˆ‚L/âˆ‚c += (âˆ‚v/âˆ‚c) Â· (âˆ‚L/âˆ‚v)
        #
        # ì—°ì‡„ ë²•ì¹™ì˜ ì§ê´€ì  ì´í•´:
        # "ìë™ì°¨ê°€ ìì „ê±°ë³´ë‹¤ 2ë°° ë¹ ë¥´ê³ , ìì „ê±°ê°€ ê±·ëŠ” ì‚¬ëŒë³´ë‹¤ 4ë°° ë¹ ë¥´ë©´,
        #  ìë™ì°¨ëŠ” ê±·ëŠ” ì‚¬ëŒë³´ë‹¤ 2 Ã— 4 = 8ë°° ë¹ ë¥´ë‹¤."
        # â†’ ê²½ë¡œë¥¼ ë”°ë¼ ë³€í™”ìœ¨ì„ ê³±í•˜ëŠ” ê²ƒì´ë‹¤.
        #
        # self.grad = 1ë¡œ ì‹œì‘: âˆ‚L/âˆ‚L = 1 (ìê¸° ìì‹ ì— ëŒ€í•œ ë³€í™”ìœ¨ì€ 1)
        #
        # í•µì‹¬ ì§ˆë¬¸: "ì™œ += ì¸ê°€?"
        # â†’ í•˜ë‚˜ì˜ ê°’ì´ ê·¸ë˜í”„ì—ì„œ ì—¬ëŸ¬ ê³³ì— ì‚¬ìš©ë  ë•Œ (ê·¸ë˜í”„ê°€ ë¶„ê¸°í•  ë•Œ),
        #   ê° ë¶„ê¸°ë¥¼ í†µí•´ ë…ë¦½ì ìœ¼ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ê°€ í˜ëŸ¬ì˜¤ê³  ì´ë¥¼ í•©ì‚°í•´ì•¼ í•œë‹¤.
        #   ì´ê²ƒì€ ë‹¤ë³€ìˆ˜ ì—°ì‡„ ë²•ì¹™ì˜ ê²°ê³¼ì´ë‹¤:
        #   cê°€ ì—¬ëŸ¬ ê²½ë¡œë¡œ Lì— ê¸°ì—¬í•˜ë©´, ì´ ë¯¸ë¶„ = ê° ê²½ë¡œ ê¸°ì—¬ì˜ í•©.
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        topo = []  # ğŸ”´ BREAKPOINT (backward ë‚´ë¶€): topo ë¦¬ìŠ¤íŠ¸ê°€ ì–´ë–¤ ìˆœì„œë¡œ ì±„ì›Œì§€ëŠ”ì§€ ê´€ì°°
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build_topo(child)
                topo.append(v)  # ğŸ”´ BREAKPOINT: topoì— ì¶”ê°€ë˜ëŠ” ë…¸ë“œ(v) í™•ì¸ â€” ìœ„ìƒ ì •ë ¬ ìˆœì„œ
        build_topo(self)
        self.grad = 1  # ğŸ”´ BREAKPOINT: loss ë…¸ë“œì˜ gradê°€ 1ë¡œ ì„¤ì •ë˜ëŠ” ìˆœê°„ (âˆ‚L/âˆ‚L = 1)
        for v in reversed(topo):
            for child, local_grad in zip(v._children, v._local_grads):
                child.grad += local_grad * v.grad  # ğŸ”´ BREAKPOINT â­: v, child, local_grad, v.grad í™•ì¸. a.grad 0â†’3â†’4 ëˆ„ì  ê´€ì°°!


# â”€â”€â”€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def show_value(name, v):
    """Valueì˜ dataì™€ gradë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    print(f"    {name:12s} â†’ data={v.data:8.4f}, grad={v.grad:8.4f}")

def show_values(label, values_dict):
    """ì—¬ëŸ¬ Valueë¥¼ í•œë²ˆì— ì¶œë ¥"""
    print(f"  [{label}]")
    for name, v in values_dict.items():
        show_value(name, v)

def separator():
    print()


# =============================================================================
# ì‹¤í—˜ 1: ë‹¨ìˆœ ì‚¬ì¹™ì—°ì‚°
# =============================================================================
#
# [explain.md ì¸ìš© ì˜ˆì œ]
# a = Value(2.0)
# b = Value(3.0)
# c = a * b       # c = 6.0
# L = c + a       # L = 8.0
# L.backward()
# print(a.grad)   # 4.0 (dL/da = b + 1 = 3 + 1, ë‘ ê²½ë¡œë¥¼ í†µí•´)
# print(b.grad)   # 2.0 (dL/db = a = 2)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("=" * 60)
print("ğŸ§ª ì‹¤í—˜ 1: ë‹¨ìˆœ ì‚¬ì¹™ì—°ì‚°")
print("=" * 60)

a = Value(2.0)
b = Value(3.0)
c = a * b       # ğŸ”´ BREAKPOINT: c._children==(a,b), c._local_grads==(3.0, 2.0) í™•ì¸
L = c + a       # ğŸ”´ BREAKPOINT: L._children==(c,a), L._local_grads==(1, 1) í™•ì¸

# â”€â”€â”€ backward() ì „: gradëŠ” ì•„ì§ ëª¨ë‘ 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n  â¸ï¸  backward() í˜¸ì¶œ ì „:")
show_values("ìˆœì „íŒŒ ê²°ê³¼", {"a": a, "b": b, "c (=a*b)": c, "L (=c+a)": L})

L.backward()  # ğŸ”´ BREAKPOINT: Step Into â†’ backward() ë‚´ë¶€ë¡œ ì§„ì…!

# â”€â”€â”€ backward() í›„: gradê°€ ê³„ì‚°ë¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n  â–¶ï¸  backward() í˜¸ì¶œ í›„:")
show_values("ì—­ì „íŒŒ ê²°ê³¼", {"a": a, "b": b, "c (=a*b)": c, "L (=c+a)": L})

# â”€â”€â”€ í•µì‹¬ ì§ˆë¬¸: a.gradê°€ ì™œ 4ì¸ê°€? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# L = c + a = (a * b) + a
#
# aëŠ” ë‘ ê³³ì—ì„œ ì‚¬ìš©ëœë‹¤ (ê·¸ë˜í”„ê°€ ë¶„ê¸°!):
#   ê²½ë¡œ 1: a â†’ a*b â†’ c â†’ L    ì´ ê²½ë¡œì˜ ê¸°ì—¬: âˆ‚L/âˆ‚c Â· âˆ‚c/âˆ‚a = 1 Â· b = 3
#   ê²½ë¡œ 2: a â†’ L               ì´ ê²½ë¡œì˜ ê¸°ì—¬: âˆ‚L/âˆ‚a = 1
#   ì´ a.grad = 3 + 1 = 4
#
# â”€â”€â”€ í•µì‹¬ ì§ˆë¬¸: b.gradê°€ ì™œ 2ì¸ê°€? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# bëŠ” í•œ ê³³ì—ì„œë§Œ ì‚¬ìš©:
#   ê²½ë¡œ: b â†’ a*b â†’ c â†’ L       ê¸°ì—¬: âˆ‚L/âˆ‚c Â· âˆ‚c/âˆ‚b = 1 Â· a = 2
#   b.grad = 2

print("""
  ğŸ’¡ í•µì‹¬ ì§ˆë¬¸: "a.gradê°€ ì™œ 4ì¸ê°€?"
     L = (a * b) + aì—ì„œ aëŠ” ë‘ ê³³ì— ì‚¬ìš©ë¨ (ê·¸ë˜í”„ ë¶„ê¸°!)
       ê²½ë¡œ 1: a â†’ a*b â†’ L  ê¸°ì—¬ = b = 3
       ê²½ë¡œ 2: a â†’ L        ê¸°ì—¬ = 1
       í•©ê³„: a.grad = 3 + 1 = 4  â† ì´ê²ƒì´ += ì¸ ì´ìœ !

  ğŸ’¡ í•µì‹¬ ì§ˆë¬¸: "b.gradê°€ ì™œ 2ì¸ê°€?"
     bëŠ” í•œ ê³³ì—ì„œë§Œ ì‚¬ìš©ë¨
       ê²½ë¡œ: b â†’ a*b â†’ L  ê¸°ì—¬ = a = 2
""")


# =============================================================================
# ì‹¤í—˜ 2: ë¶„ê¸° ê·¸ë˜í”„ (í•˜ë‚˜ì˜ ê°’ì´ ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©ë˜ëŠ” ê²½ìš°)
# =============================================================================
#
# í•µì‹¬ ì§ˆë¬¸: "ì™œ += ì¸ê°€?"
# â†’ í•˜ë‚˜ì˜ ê°’ì´ ê·¸ë˜í”„ì—ì„œ ì—¬ëŸ¬ ê³³ì— ì‚¬ìš©ë˜ë©´,
#   ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ê° ì‚¬ìš©ì²˜ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ í˜ëŸ¬ì™€ì„œ í•©ì‚°ë˜ì–´ì•¼ í•œë‹¤.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("=" * 60)
print("ğŸ§ª ì‹¤í—˜ 2: ë¶„ê¸° ê·¸ë˜í”„ â€” ì™œ += ì¸ê°€?")
print("=" * 60)

a = Value(3.0)
# aê°€ 3ë²ˆ ì‚¬ìš©ë˜ëŠ” ê²½ìš°
y1 = a * a      # ğŸ”´ BREAKPOINT: ê°™ì€ a ê°ì²´ê°€ _childrenì— 2ë²ˆ ë“¤ì–´ê°€ëŠ”ì§€ í™•ì¸
y2 = a + a      # ğŸ”´ BREAKPOINT: ì—¬ê¸°ì„œë„ ê°™ì€ a ê°ì²´ê°€ _childrenì— 2ë²ˆ
L = y1 + y2     # L = aÂ² + 2a = 9 + 6 = 15.0

print("\n  â¸ï¸  backward() í˜¸ì¶œ ì „:")
show_values("ìˆœì „íŒŒ ê²°ê³¼", {"a": a, "y1 (=a*a)": y1, "y2 (=a+a)": y2, "L": L})

L.backward()

print("\n  â–¶ï¸  backward() í˜¸ì¶œ í›„:")
show_values("ì—­ì „íŒŒ ê²°ê³¼", {"a": a, "y1 (=a*a)": y1, "y2 (=a+a)": y2, "L": L})

# â”€â”€â”€ ìˆ˜ë™ ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# L = aÂ² + 2a
# dL/da = 2a + 2 = 2(3) + 2 = 8
# a.gradê°€ 8ì´ ë‚˜ì™”ëŠ”ê°€?

print(f"""
  ğŸ’¡ ìˆ˜ë™ ê²€ì¦: L = aÂ² + 2a
     dL/da = 2a + 2 = 2Â·3 + 2 = 8
     a.grad = {a.grad} â† {'âœ… ì •ë‹µ!' if a.grad == 8.0 else 'âŒ ì˜¤ë‹µ!'}

  ğŸ’¡ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ê³¼ì • ì¶”ì :
     ì´ˆê¸°: a.grad = 0
     y1 = a * a ì—ì„œ:
       aëŠ” ì™¼ìª½ ì…ë ¥ìœ¼ë¡œ: a.grad += a.data(ì˜¤ë¥¸ìª½) Â· y1.grad = 3 Â· 1 = 3
       aëŠ” ì˜¤ë¥¸ìª½ ì…ë ¥ìœ¼ë¡œ: a.grad += a.data(ì™¼ìª½) Â· y1.grad = 3 Â· 1 = 3
       â†’ a.grad = 6 (aÂ² ë¯¸ë¶„ = 2a = 6ê³¼ ì¼ì¹˜!)
     y2 = a + a ì—ì„œ:
       aëŠ” ì™¼ìª½ ì…ë ¥ìœ¼ë¡œ: a.grad += 1 Â· y2.grad = 1 Â· 1 = 1
       aëŠ” ì˜¤ë¥¸ìª½ ì…ë ¥ìœ¼ë¡œ: a.grad += 1 Â· y2.grad = 1 Â· 1 = 1
       â†’ a.grad = 6 + 2 = 8
     ìµœì¢…: a.grad = 8 âœ…
""")


# =============================================================================
# ì‹¤í—˜ 3: ëº„ì…ˆê³¼ ë‚˜ëˆ—ì…ˆë„ autogradê°€ ì²˜ë¦¬í•˜ëŠ”ê°€?
# =============================================================================

print("=" * 60)
print("ğŸ§ª ì‹¤í—˜ 3: ëº„ì…ˆê³¼ ë‚˜ëˆ—ì…ˆ")
print("=" * 60)

a = Value(10.0)
b = Value(3.0)
c = a - b       # c = 10 - 3 = 7    (ë‚´ë¶€ì ìœ¼ë¡œ a + (-b) = a + (b * -1))
d = a / b       # d = 10 / 3 â‰ˆ 3.33 (ë‚´ë¶€ì ìœ¼ë¡œ a * (b ** -1))
L = c + d       # L = 7 + 3.33 = 10.33

print("\n  â¸ï¸  backward() í˜¸ì¶œ ì „:")
show_values("ìˆœì „íŒŒ ê²°ê³¼", {"a": a, "b": b, "c (=a-b)": c, "d (=a/b)": d, "L": L})

L.backward()

print("\n  â–¶ï¸  backward() í˜¸ì¶œ í›„:")
show_values("ì—­ì „íŒŒ ê²°ê³¼", {"a": a, "b": b, "c (=a-b)": c, "d (=a/b)": d, "L": L})

# â”€â”€â”€ ìˆ˜ë™ ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# L = (a - b) + (a / b) = (a - b) + aÂ·b^(-1)
# dL/da = 1 + 1/b = 1 + 1/3 â‰ˆ 1.3333
# dL/db = -1 + aÂ·(-1)Â·b^(-2) = -1 - a/bÂ² = -1 - 10/9 â‰ˆ -2.1111
expected_a_grad = 1 + 1/b.data
expected_b_grad = -1 - a.data / (b.data**2)
# ì°¸ê³ : a.dataì™€ b.dataëŠ” backward ê³¼ì •ì—ì„œ ë°”ë€Œì§€ ì•Šìœ¼ë¯€ë¡œ ì›ë˜ ê°’ ì‚¬ìš©

print(f"""
  ğŸ’¡ ìˆ˜ë™ ê²€ì¦: L = (a - b) + (a / b)
     dL/da = 1 + 1/b = 1 + 1/3 â‰ˆ {expected_a_grad:.4f}
     a.grad = {a.grad:.4f} â† {'âœ…' if abs(a.grad - expected_a_grad) < 1e-6 else 'âŒ'}

     dL/db = -1 - a/bÂ² = -1 - 10/9 â‰ˆ {expected_b_grad:.4f}
     b.grad = {b.grad:.4f} â† {'âœ…' if abs(b.grad - expected_b_grad) < 1e-6 else 'âŒ'}

  ğŸ’¡ ì°¸ê³ : __sub__ê³¼ __truediv__ëŠ” Value í´ë˜ìŠ¤ì— ì§ì ‘ êµ¬í˜„ë˜ì–´ ìˆì§€ ì•Šê³ ,
     - __sub__: self + (-other) â†’ __add__ì™€ __neg__ ì¡°í•©
     - __truediv__: self * other**-1 â†’ __mul__ì™€ __pow__ ì¡°í•©
     â†’ autogradëŠ” ê¸°ë³¸ ì—°ì‚° ë¸”ë¡ë§Œìœ¼ë¡œ ë³µì¡í•œ ì—°ì‚°ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ìë™ ê³„ì‚°í•œë‹¤!
""")


# =============================================================================
# ì‹¤í—˜ 4: ReLUë¥¼ í¬í•¨í•œ ê·¸ë˜í”„
# =============================================================================

print("=" * 60)
print("ğŸ§ª ì‹¤í—˜ 4: ReLU â€” ì¡°ê±´ë¶€ ê·¸ë˜ë””ì–¸íŠ¸")
print("=" * 60)

# ì–‘ìˆ˜ ì…ë ¥
a_pos = Value(2.0)
r_pos = a_pos.relu()  # max(0, 2.0) = 2.0

r_pos.backward()
print(f"\n  [ì–‘ìˆ˜ ì…ë ¥] a = 2.0")
show_values("backward í›„", {"a": a_pos, "relu(a)": r_pos})
print(f"    â†’ ì–‘ìˆ˜ì´ë¯€ë¡œ reluëŠ” í†µê³¼: grad = {a_pos.grad} (= 1.0)")

# ìŒìˆ˜ ì…ë ¥
a_neg = Value(-3.0)
r_neg = a_neg.relu()  # max(0, -3.0) = 0.0

r_neg.backward()
print(f"\n  [ìŒìˆ˜ ì…ë ¥] a = -3.0")
show_values("backward í›„", {"a": a_neg, "relu(a)": r_neg})
print(f"    â†’ ìŒìˆ˜ì´ë¯€ë¡œ reluê°€ ì°¨ë‹¨: grad = {a_neg.grad} (= 0.0)")

print("""
  ğŸ’¡ ReLUì˜ ë¡œì»¬ ê·¸ë˜ë””ì–¸íŠ¸:
     a > 0 â†’ 1.0 (ê·¸ë˜ë””ì–¸íŠ¸ê°€ ê·¸ëŒ€ë¡œ í†µê³¼)
     a â‰¤ 0 â†’ 0.0 (ê·¸ë˜ë””ì–¸íŠ¸ ì°¨ë‹¨ = "ì£½ì€ ë‰´ëŸ°")

  ğŸ’¡ ì‹ ê²½ë§ì—ì„œì˜ ì—­í• :
     ReLUëŠ” ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¡œ, MLP ë¸”ë¡ì—ì„œ ì‚¬ìš©ëœë‹¤.
     ì„ í˜• ë³€í™˜ë§Œìœ¼ë¡œëŠ” ì•„ë¬´ë¦¬ ìŒ“ì•„ë„ ê²°êµ­ í•˜ë‚˜ì˜ ì„ í˜• ë³€í™˜ì´ë¯€ë¡œ,
     ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ë¼ì›Œë„£ì–´ì•¼ ë” ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
""")


# =============================================================================
# ì‹¤í—˜ 5: Softmax + Cross-Entropy Loss
# =============================================================================
#
# [explain.md - Training loop ì„¹ì…˜]
# ëª¨ë¸ì˜ ì¶œë ¥(logits)ì„ softmaxë¡œ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•˜ê³ ,
# ì •ë‹µ í† í°ì˜ í™•ë¥ ì— -logë¥¼ ì·¨í•´ ì†ì‹¤ì„ ê³„ì‚° (cross-entropy loss)
#
# ì§ê´€: ì†ì‹¤ = ëª¨ë¸ì´ ì‹¤ì œ ë‹¤ìŒ í† í°ì— ì–¼ë§ˆë‚˜ "ë†€ëëŠ”ê°€"ì˜ ì •ë„
# - ì •ë‹µì— í™•ë¥  1.0 í• ë‹¹? â†’ ì†ì‹¤ = 0 (ì „í˜€ ë†€ë¼ì§€ ì•ŠìŒ)
# - ì •ë‹µì— í™•ë¥  ~0.0 í• ë‹¹? â†’ ì†ì‹¤ â†’ +âˆ (ë§¤ìš° ë†€ëŒ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("=" * 60)
print("ğŸ§ª ì‹¤í—˜ 5: Softmax + Cross-Entropy Loss")
print("=" * 60)

# ì´ í•¨ìˆ˜ë“¤ì€ microgpt.pyì—ì„œ ê°€ì ¸ì˜¨ ê²ƒ
def softmax(logits):
    """ë¡œì§“(logits)ì„ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜"""
    max_val = max(val.data for val in logits)           # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ“ê°’ ë¹¼ê¸°
    exps = [(val - max_val).exp() for val in logits]    # e^(x - max)
    total = sum(exps)                                    # ì •ê·œí™” ìƒìˆ˜
    return [e / total for e in exps]                     # í™•ë¥  (í•© = 1)

# ì‹œë‚˜ë¦¬ì˜¤: 3ê°œ í† í°(a=0, b=1, c=2)ì— ëŒ€í•œ logits
# ì •ë‹µì€ í† í° 1 (b)
logits = [Value(2.0), Value(5.0), Value(1.0)]  # ëª¨ë¸ì´ í† í° 1(b)ì— ë†’ì€ ì ìˆ˜ ë¶€ì—¬
target = 1  # ì •ë‹µ í† í° ID

print(f"\n  ì…ë ¥ logits: [{', '.join(f'{l.data:.1f}' for l in logits)}]")
print(f"  ì •ë‹µ í† í° ID: {target}")

# Softmax
probs = softmax(logits)  # ğŸ”´ BREAKPOINT: probs ê° ì›ì†Œì˜ .data í•©ì´ 1.0ì¸ì§€
print(f"\n  [Softmax ê²°ê³¼ â€” í™•ë¥  ë¶„í¬]")
for i, p in enumerate(probs):
    marker = " â† ì •ë‹µ!" if i == target else ""
    print(f"    í† í° {i}: logit={logits[i].data:5.1f} â†’ prob={p.data:.4f}{marker}")
prob_sum = sum(p.data for p in probs)
print(f"    í™•ë¥  í•©ê³„: {prob_sum:.6f} (â‰ˆ 1.0ì´ì–´ì•¼ í•¨)")

# Cross-entropy loss
loss = -probs[target].log()  # ğŸ”´ BREAKPOINT: loss ê°’ì´ ì‘ì„ìˆ˜ë¡ ì˜ˆì¸¡ ì •í™•
print(f"\n  [Cross-Entropy Loss]")
print(f"    loss = -log(prob[{target}]) = -log({probs[target].data:.4f}) = {loss.data:.4f}")

# â”€â”€â”€ backward() ì „ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n  â¸ï¸  backward() í˜¸ì¶œ ì „:")
for i, l in enumerate(logits):
    print(f"    logits[{i}]: data={l.data:6.2f}, grad={l.grad:6.4f}")

loss.backward()  # ğŸ”´ BREAKPOINT: backward í›„ logits[1].grad < 0, ë‚˜ë¨¸ì§€ > 0 í™•ì¸

# â”€â”€â”€ backward() í›„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n  â–¶ï¸  backward() í˜¸ì¶œ í›„:")
for i, l in enumerate(logits):
    print(f"    logits[{i}]: data={l.data:6.2f}, grad={l.grad:+6.4f}")

print("""
  ğŸ’¡ ê·¸ë˜ë””ì–¸íŠ¸ í•´ì„:
     - logits[1] (ì •ë‹µ)ì˜ grad < 0: ì´ ê°’ì„ ë†’ì´ë©´ lossê°€ ì¤„ì–´ë“ ë‹¤!
       â†’ ì˜µí‹°ë§ˆì´ì €ê°€ ì´ ë°©í–¥ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸
     - logits[0], logits[2] (ì˜¤ë‹µ)ì˜ grad > 0: ì´ ê°’ì„ ë†’ì´ë©´ lossê°€ ì¦ê°€
       â†’ ì˜µí‹°ë§ˆì´ì €ê°€ ì´ë“¤ì„ ë‚®ì¶”ëŠ” ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸

  ğŸ’¡ ì´ê²ƒì´ í•™ìŠµì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì´ë‹¤:
     forward â†’ ì˜ˆì¸¡ â†’ loss ê³„ì‚° â†’ backward â†’ ê·¸ë˜ë””ì–¸íŠ¸ íšë“ â†’ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
     ì´ ë£¨í”„ë¥¼ ë°˜ë³µí•˜ë©´ ëª¨ë¸ì´ ì ì  ë” ì •í™•í•œ ì˜ˆì¸¡ì„ í•˜ê²Œ ëœë‹¤.
""")

# â”€â”€â”€ ë³´ë„ˆìŠ¤: ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("  [ë¹„êµ ì‹¤í—˜: ëª¨ë¸ì´ ì •ë‹µì„ ì´ë¯¸ ì˜ ë§ì¶”ëŠ” ê²½ìš° vs ëª» ë§ì¶”ëŠ” ê²½ìš°]")

scenarios = [
    ("í™•ì‹  O (ì •ë‹µì— ë†’ì€ ì ìˆ˜)", [Value(-1.0), Value(10.0), Value(-1.0)], 1),
    ("í™•ì‹  X (ê· ë“±í•œ ì ìˆ˜)",     [Value(1.0),  Value(1.0),  Value(1.0)],  1),
    ("ì™„ì „íˆ í‹€ë¦¼",              [Value(10.0), Value(-5.0), Value(-5.0)], 1),
]

for desc, lgt, tgt in scenarios:
    p = softmax(lgt)
    ls = -p[tgt].log()
    print(f"    {desc:30s} | prob[ì •ë‹µ]={p[tgt].data:.6f} | loss={ls.data:.4f}")

print("""
  ğŸ’¡ ê´€ì°°:
     - ì •ë‹µ í™•ë¥ ì´ ë†’ì„ìˆ˜ë¡ lossê°€ 0ì— ê°€ê¹Œì›Œì§„ë‹¤
     - ì •ë‹µ í™•ë¥ ì´ ë‚®ì„ìˆ˜ë¡ lossê°€ ì»¤ì§„ë‹¤
     - loss = -log(1.0) = 0 (ì™„ë²½í•œ ì˜ˆì¸¡)
     - loss = -log(1/3) â‰ˆ 1.0986 (ëœë¤ ì¶”ì¸¡)
     - loss = -log(0.0000...) â†’ +âˆ (ì™„ì „íˆ í‹€ë¦° ì˜ˆì¸¡)

     microgptì—ì„œ 27ê°œ í† í° ëœë¤ ì¶”ì¸¡ì˜ loss:
     -log(1/27) â‰ˆ 3.2958 (í•™ìŠµ ì‹œì‘ ì‹œ lossê°€ ~3.3ì¸ ì´ìœ !)
""")


# =============================================================================
# ì‹¤í—˜ 6: ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ autograd ê²€ì¦í•˜ê¸°
# =============================================================================

print("=" * 60)
print("ğŸ§ª ì‹¤í—˜ 6: ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ autograd ê²€ì¦")
print("=" * 60)

def numerical_gradient(func, x, h=1e-5):
    """ìˆ˜ì¹˜ ë¯¸ë¶„: f'(x) â‰ˆ (f(x+h) - f(x-h)) / (2h)"""
    return (func(x + h) - func(x - h)) / (2 * h)

# f(x) = xÂ³ + 2xÂ² - 5x + 3
# f'(x) = 3xÂ² + 4x - 5
x_val = 2.0

# Autogradë¡œ ê³„ì‚°
x = Value(x_val)
f = x**3 + 2 * x**2 - 5 * x + 3  # f(2) = 8 + 8 - 10 + 3 = 9
f.backward()
autograd_result = x.grad

# ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ ê³„ì‚°
numerical_result = numerical_gradient(
    lambda v: v**3 + 2 * v**2 - 5 * v + 3,
    x_val
)

# í•´ì„ì  ë¯¸ë¶„ìœ¼ë¡œ ê³„ì‚°
# f'(x) = 3xÂ² + 4x - 5
# f'(2) = 12 + 8 - 5 = 15
analytic_result = 3 * x_val**2 + 4 * x_val - 5

print(f"""
  f(x) = xÂ³ + 2xÂ² - 5x + 3,  x = {x_val}

  ë°©ë²• 1) Autograd:     f'({x_val}) = {autograd_result:.6f}
  ë°©ë²• 2) ìˆ˜ì¹˜ ë¯¸ë¶„:    f'({x_val}) = {numerical_result:.6f}
  ë°©ë²• 3) í•´ì„ì  ë¯¸ë¶„:  f'({x_val}) = {analytic_result:.6f}

  Autograd â‰ˆ ìˆ˜ì¹˜ë¯¸ë¶„? {'âœ… Yes!' if abs(autograd_result - numerical_result) < 1e-4 else 'âŒ No!'}
  Autograd = í•´ì„ì ?   {'âœ… Yes!' if abs(autograd_result - analytic_result) < 1e-6 else 'âŒ No!'}

  ğŸ’¡ ìˆ˜ì¹˜ ë¯¸ë¶„ì€ ëŠë¦¬ì§€ë§Œ autograd êµ¬í˜„ì„ ê²€ì¦í•˜ëŠ” "ê¸ˆ í‘œì¤€(gold standard)"ì´ë‹¤.
     PyTorchì—ë„ torch.autograd.gradcheck()ì´ë¼ëŠ” í•¨ìˆ˜ê°€ ìˆì–´ì„œ
     autograd ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ ê²€ì¦í•œë‹¤.
""")


# =============================================================================
# í•µì‹¬ ì •ë¦¬
# =============================================================================
print("=" * 60)
print("ğŸ¯ í•µì‹¬ ì •ë¦¬")
print("=" * 60)
print("""
  1. Value í´ë˜ìŠ¤: ìŠ¤ì¹¼ë¼ë¥¼ ê°ì‹¸ê³  ê³„ì‚° ê³¼ì •ì„ ì¶”ì í•˜ëŠ” autograd ë…¸ë“œ
  2. ìˆœì „íŒŒ(forward): ì—°ì‚° ìˆ˜í–‰ + ê³„ì‚° ê·¸ë˜í”„ êµ¬ì¶•
  3. ì—­ì „íŒŒ(backward): ì—°ì‡„ ë²•ì¹™ìœ¼ë¡œ ëª¨ë“  ë…¸ë“œì˜ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
  4. += (ëˆ„ì ): ê·¸ë˜í”„ ë¶„ê¸° ì‹œ ê° ê²½ë¡œì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ í•©ì‚°
  5. ë¡œì»¬ ê·¸ë˜ë””ì–¸íŠ¸: ê° ì—°ì‚°ì´ ìì‹ ì˜ ë¯¸ë¶„ ê³µì‹ì„ "ì•Œê³ " ìˆìŒ
  6. Softmax + Cross-Entropy: ëª¨ë¸ ì¶œë ¥ â†’ í™•ë¥  â†’ ì†ì‹¤ â†’ ì—­ì „íŒŒ
  7. ì´ ëª¨ë“  ê²ƒì´ PyTorchì˜ loss.backward()ê°€ í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•œ ì•Œê³ ë¦¬ì¦˜!
     (ë‹¤ë§Œ ìŠ¤ì¹¼ë¼ ëŒ€ì‹  í…ì„œ, Python ëŒ€ì‹  CUDA ì»¤ë„)

  ğŸ’¡ ë‹¤ìŒ ì±•í„°ì—ì„œ ì´ autogradë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¨ë‹¤.
""")
