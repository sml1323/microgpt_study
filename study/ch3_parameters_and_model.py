"""
=============================================================================
 Chapter 3: Parameters & Model Architecture (íŒŒë¼ë¯¸í„°ì™€ ëª¨ë¸ êµ¬ì¡°)
 - microgpt.py L73~143ì— í•´ë‹¹í•˜ëŠ” ì½”ë“œ
 - explain.mdì˜ Parameters, Architecture ì„¹ì…˜ ë‚´ìš©ì„ í•œêµ­ì–´ ì£¼ì„ìœ¼ë¡œ í¬í•¨
 - ë””ë²„ê·¸ìš© ì¶•ì†Œ íŒŒë¼ë¯¸í„°: n_embd=4, n_head=2, n_layer=1
 - ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥: python study/ch3_parameters_and_model.py
=============================================================================
"""

import os
import math
import random
random.seed(42)

# ë””ë²„ê±°(LazyVim ë“±)ì—ì„œ cwdê°€ study/ì¼ ë•Œ input.txtë¥¼ ì°¾ê¸° ìœ„í•´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
os.chdir(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì „ ì¤€ë¹„: Value í´ë˜ìŠ¤ (ch2ì—ì„œ ê°€ì ¸ì˜´, ë…ë¦½ ì‹¤í–‰ì„ ìœ„í•´ í¬í•¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Value:
    __slots__ = ('data', 'grad', '_children', '_local_grads')

    def __init__(self, data, children=(), local_grads=()):
        self.data = data
        self.grad = 0
        self._children = children
        self._local_grads = local_grads

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data + other.data, (self, other), (1, 1))

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data * other.data, (self, other), (other.data, self.data))

    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))
    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))
    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))
    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))
    def __neg__(self): return self * -1
    def __radd__(self, other): return self + other
    def __sub__(self, other): return self + (-other)
    def __rsub__(self, other): return other + (-self)
    def __rmul__(self, other): return self * other
    def __truediv__(self, other): return self * other**-1
    def __rtruediv__(self, other): return other * self**-1

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        self.grad = 1
        for v in reversed(topo):
            for child, local_grad in zip(v._children, v._local_grads):
                child.grad += local_grad * v.grad

    def __repr__(self):
        return f"Value(data={self.data:.4f})"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì „ ì¤€ë¹„: í† í¬ë‚˜ì´ì € (ch1ì—ì„œ ê°€ì ¸ì˜´, ë…ë¦½ ì‹¤í–‰ì„ ìœ„í•´ í¬í•¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not os.path.exists('input.txt'):
    import urllib.request
    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt'
    urllib.request.urlretrieve(names_url, 'input.txt')
docs = [line.strip() for line in open('input.txt') if line.strip()]
random.shuffle(docs)

uchars = sorted(set(''.join(docs)))
BOS = len(uchars)
vocab_size = len(uchars) + 1
print(f"vocab_size: {vocab_size}")


# =============================================================================
# ì„¹ì…˜ 1: íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” (microgpt.py L73~89)
# =============================================================================
#
# [explain.md - Parameters ì„¹ì…˜ ìš”ì•½]
# íŒŒë¼ë¯¸í„°ëŠ” ëª¨ë¸ì˜ ì§€ì‹ì´ë‹¤.
# ë¶€ë™ì†Œìˆ˜ì  ìˆ«ìë“¤(Valueë¡œ ê°ì‹¸ì§„)ì˜ í° ì§‘í•©ìœ¼ë¡œ,
# ì²˜ìŒì—ëŠ” ëœë¤ìœ¼ë¡œ ì‹œì‘í•˜ê³  í•™ìŠµ ì¤‘ì— ë°˜ë³µì ìœ¼ë¡œ ìµœì í™”ëœë‹¤.
#
# ê° íŒŒë¼ë¯¸í„°ëŠ” ê°€ìš°ì‹œì•ˆ ë¶„í¬ì—ì„œ ë½‘ì€ ì‘ì€ ëœë¤ ìˆ«ìë¡œ ì´ˆê¸°í™”ëœë‹¤.
# state_dictëŠ” ì´ë¦„ ë¶™ì¸ í–‰ë ¬ë“¤ë¡œ êµ¬ì„±:
#   - ì„ë² ë”© í…Œì´ë¸” (wte, wpe)
#   - ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (attn_wq, attn_wk, attn_wv, attn_wo)
#   - MLP ê°€ì¤‘ì¹˜ (mlp_fc1, mlp_fc2)
#   - ì¶œë ¥ í”„ë¡œì ì…˜ (lm_head)
#
# ìš°ë¦¬ì˜ ì¶•ì†Œ ëª¨ë¸: ì•½ ìˆ˜ë°± ê°œ íŒŒë¼ë¯¸í„°
# ì›ë³¸ microgpt: 4,192ê°œ íŒŒë¼ë¯¸í„°
# GPT-2: 16ì–µ ê°œ, í˜„ëŒ€ LLM: ìˆ˜ì²œì–µ ê°œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n" + "=" * 60)
print("ğŸ”§ [ì„¹ì…˜ 1] íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”")
print("=" * 60)

# --- ğŸ’¡ ë””ë²„ê·¸ìš© ì¶•ì†Œ íŒŒë¼ë¯¸í„° ---
# ì›ë³¸: n_layer=1, n_embd=16, block_size=16, n_head=4
# ì¶•ì†Œ: n_layer=1, n_embd=4,  block_size=8,  n_head=2
n_layer = 1       # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ìˆ˜ (ê¹Šì´)
n_embd = 4        # ì„ë² ë”© ì°¨ì› (ë„ˆë¹„) â† ì›ë³¸ 16ì—ì„œ 4ë¡œ ì¶•ì†Œ
block_size = 8    # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ â† ì›ë³¸ 16ì—ì„œ 8ë¡œ ì¶•ì†Œ
n_head = 2        # ì–´í…ì…˜ í—¤ë“œ ìˆ˜ â† ì›ë³¸ 4ì—ì„œ 2ë¡œ ì¶•ì†Œ
head_dim = n_embd // n_head  # ê° í—¤ë“œì˜ ì°¨ì› = 4 // 2 = 2

print(f"  n_layer    = {n_layer}")
print(f"  n_embd     = {n_embd}")
print(f"  block_size = {block_size}")
print(f"  n_head     = {n_head}")
print(f"  head_dim   = {head_dim} (= n_embd // n_head = {n_embd} // {n_head})")

# --- ì›ë³¸ ì½”ë“œ (microgpt.py L79~88) ---
# matrix: noutÃ—nin í¬ê¸°ì˜ 2D ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ” ëŒë‹¤ í•¨ìˆ˜
# ê° ì›ì†ŒëŠ” Value(ê°€ìš°ì‹œì•ˆ ëœë¤), std=0.08ë¡œ ì‘ì€ ê°’ì—ì„œ ì‹œì‘
matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]

state_dict = {
    'wte': matrix(vocab_size, n_embd),      # í† í° ì„ë² ë”© (27 Ã— 4)
    'wpe': matrix(block_size, n_embd),       # ìœ„ì¹˜ ì„ë² ë”© (8 Ã— 4)
    'lm_head': matrix(vocab_size, n_embd),   # ì¶œë ¥ í”„ë¡œì ì…˜ (27 Ã— 4)
}
for i in range(n_layer):
    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)    # ì¿¼ë¦¬ ê°€ì¤‘ì¹˜ (4 Ã— 4)
    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)    # í‚¤ ê°€ì¤‘ì¹˜   (4 Ã— 4)
    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)    # ê°’ ê°€ì¤‘ì¹˜   (4 Ã— 4)
    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)    # ì¶œë ¥ ê°€ì¤‘ì¹˜ (4 Ã— 4)
    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd) # MLP ì—… (16 Ã— 4)
    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd) # MLP ë‹¤ìš´ (4 Ã— 16)

params = [p for mat in state_dict.values() for row in mat for p in row]
print(f"\n  ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {len(params)}")

# â”€â”€â”€ ë””ë²„ê·¸ í¬ì¸íŠ¸: state_dict êµ¬ì¡°ì™€ ê° í–‰ë ¬ì˜ shape â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n  [state_dict êµ¬ì¡° â€” í–‰ë ¬ ì´ë¦„ê³¼ shape]")
for name, mat in state_dict.items():
    rows = len(mat)
    cols = len(mat[0]) if mat else 0
    num_params = rows * cols
    # ì²« ë²ˆì§¸ ì›ì†Œì˜ ì‹¤ì œ ê°’ë„ ë³´ì—¬ì¤Œ
    sample_val = mat[0][0].data if mat and mat[0] else None
    print(f"    {name:25s} â†’ shape ({rows:2d} Ã— {cols:2d}) = {num_params:4d} params  (ì˜ˆ: {sample_val:+.4f})")

# â”€â”€â”€ í•µì‹¬ ê´€ì°°: ì„ë² ë”© í…Œì´ë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n  [í† í° ì„ë² ë”© ê´€ì°° â€” wteì—ì„œ BOSì˜ ë²¡í„°]")
bos_emb = state_dict['wte'][BOS]
print(f"    wte[BOS={BOS}] = [{', '.join(f'{v.data:+.4f}' for v in bos_emb)}]")
print(f"    â†’ ì´ {n_embd}ì°¨ì› ë²¡í„°ê°€ BOS í† í°ì˜ 'ì‹ ê²½ ì„œëª…(neural signature)'")

a_emb = state_dict['wte'][0]  # 'a' = 0
print(f"    wte['a'=0]  = [{', '.join(f'{v.data:+.4f}' for v in a_emb)}]")
print(f"    â†’ ì²˜ìŒì—” ëœë¤ì´ì§€ë§Œ, í•™ìŠµí•˜ë©´ì„œ ì˜ë¯¸ ìˆëŠ” í‘œí˜„ìœ¼ë¡œ ë³€í•œë‹¤")


# =============================================================================
# ì„¹ì…˜ 2: ëª¨ë¸ ì•„í‚¤í…ì²˜ â€” í—¬í¼ í•¨ìˆ˜ë“¤ (microgpt.py L93~105)
# =============================================================================
#
# [explain.md - Architecture ì„¹ì…˜ ìš”ì•½]
# ëª¨ë¸ ì•„í‚¤í…ì²˜ëŠ” ìƒíƒœ ì—†ëŠ”(stateless) í•¨ìˆ˜:
# í† í°, ìœ„ì¹˜, íŒŒë¼ë¯¸í„°, ì´ì „ ìœ„ì¹˜ì˜ ìºì‹œëœ key/valueë¥¼ ë°›ì•„ì„œ
# ë‹¤ìŒì— ì˜¬ í† í°ì— ëŒ€í•œ ë¡œì§“(ì ìˆ˜)ì„ ë°˜í™˜í•œë‹¤.
#
# GPT-2ë¥¼ ë”°ë¥´ë˜ ì•½ê°„ì˜ ë‹¨ìˆœí™”:
# - LayerNorm â†’ RMSNorm
# - bias ì—†ìŒ
# - GeLU â†’ ReLU
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n\n" + "=" * 60)
print("ğŸ“ [ì„¹ì…˜ 2] í—¬í¼ í•¨ìˆ˜: linear, softmax, rmsnorm")
print("=" * 60)

# --- ì›ë³¸ ì½”ë“œ (microgpt.py L93~94) ---
# [explain.md ì„¤ëª…]
# linearì€ í–‰ë ¬-ë²¡í„° ê³±ì…ˆì´ë‹¤.
# ë²¡í„° xì™€ ê°€ì¤‘ì¹˜ í–‰ë ¬ wë¥¼ ë°›ì•„, wì˜ ê° í–‰ê³¼ xì˜ ë‚´ì ì„ ê³„ì‚°í•œë‹¤.
# ì´ê²ƒì´ ì‹ ê²½ë§ì˜ ê¸°ë³¸ ë¹Œë”© ë¸”ë¡: í•™ìŠµëœ ì„ í˜• ë³€í™˜.
def linear(x, w):
    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]

# --- ì›ë³¸ ì½”ë“œ (microgpt.py L96~100) ---
# [explain.md ì„¤ëª…]
# softmaxëŠ” ë¡œì§“(raw score) ë²¡í„°ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜:
# - ëª¨ë“  ê°’ì´ [0, 1] ì‚¬ì´ë¡œ ê°€ê³ , í•©ì´ 1ì´ ëœë‹¤.
# - ìµœëŒ“ê°’ì„ ë¨¼ì € ë¹¼ëŠ” ì´ìœ : ìˆ˜ì¹˜ ì•ˆì •ì„± (expì˜ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€)
# - ìˆ˜í•™ì ìœ¼ë¡œëŠ” ê²°ê³¼ê°€ ë™ì¼í•˜ë‹¤.
def softmax(logits):
    max_val = max(val.data for val in logits)
    exps = [(val - max_val).exp() for val in logits]
    total = sum(exps)
    return [e / total for e in exps]

# --- ì›ë³¸ ì½”ë“œ (microgpt.py L102~105) ---
# [explain.md ì„¤ëª…]
# rmsnorm (Root Mean Square Normalization):
# ë²¡í„°ë¥¼ ë‹¨ìœ„ RMSë¥¼ ê°–ë„ë¡ ì¬ì¡°ì •(rescale)í•œë‹¤.
# í™œì„±ê°’ì´ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µê³¼í•˜ë©´ì„œ ì»¤ì§€ê±°ë‚˜ ì¤„ì–´ë“œëŠ” ê²ƒì„ ë°©ì§€ â†’ í•™ìŠµ ì•ˆì •í™”.
# ì›ë³¸ GPT-2ì˜ LayerNormì˜ ë” ë‹¨ìˆœí•œ ë³€í˜•ì´ë‹¤.
def rmsnorm(x):
    ms = sum(xi * xi for xi in x) / len(x)
    scale = (ms + 1e-5) ** -0.5
    return [xi * scale for xi in x]

# â”€â”€â”€ ë””ë²„ê·¸ í¬ì¸íŠ¸: ê° í—¬í¼ í•¨ìˆ˜ì˜ ì…ì¶œë ¥ ê´€ì°° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# linear í…ŒìŠ¤íŠ¸
print(f"\n  [linear í•¨ìˆ˜ ê´€ì°°]")
test_x = [Value(1.0), Value(2.0), Value(3.0)]
test_w = [[Value(0.1), Value(0.2), Value(0.3)],
           [Value(0.4), Value(0.5), Value(0.6)]]
test_out = linear(test_x, test_w)
print(f"    ì…ë ¥ x:  [{', '.join(f'{v.data:.1f}' for v in test_x)}]  (ê¸¸ì´ {len(test_x)})")
print(f"    ê°€ì¤‘ì¹˜ w: {len(test_w)}Ã—{len(test_w[0])} í–‰ë ¬")
print(f"    ì¶œë ¥:    [{', '.join(f'{v.data:.2f}' for v in test_out)}]  (ê¸¸ì´ {len(test_out)})")
print(f"    ê²€ì¦: w[0]Â·x = 0.1Ã—1 + 0.2Ã—2 + 0.3Ã—3 = {0.1*1 + 0.2*2 + 0.3*3:.1f} âœ“")
print(f"    ğŸ’¡ ì¶œë ¥ ê¸¸ì´ = wì˜ í–‰ ìˆ˜ (nout). ì…ë ¥ì„ ë‹¤ë¥¸ ì°¨ì›ìœ¼ë¡œ 'íˆ¬ì˜(project)'í•˜ëŠ” ê²ƒ!")

# softmax í…ŒìŠ¤íŠ¸
print(f"\n  [softmax í•¨ìˆ˜ ê´€ì°°]")
test_logits = [Value(2.0), Value(5.0), Value(1.0)]
test_probs = softmax(test_logits)
print(f"    ì…ë ¥ logits: [{', '.join(f'{v.data:.1f}' for v in test_logits)}]")
print(f"    ì¶œë ¥ probs:  [{', '.join(f'{v.data:.4f}' for v in test_probs)}]")
print(f"    í•©ê³„: {sum(p.data for p in test_probs):.6f} (= 1.0ì´ì–´ì•¼ í•¨)")
print(f"    ğŸ’¡ ê°€ì¥ í° logit(5.0) â†’ ê°€ì¥ ë†’ì€ í™•ë¥ ({max(p.data for p in test_probs):.4f})")

# rmsnorm í…ŒìŠ¤íŠ¸
print(f"\n  [rmsnorm í•¨ìˆ˜ ê´€ì°°]")
test_vec = [Value(3.0), Value(4.0), Value(0.0), Value(1.0)]
normed = rmsnorm(test_vec)
print(f"    ì…ë ¥:  [{', '.join(f'{v.data:.1f}' for v in test_vec)}]")
print(f"    ì¶œë ¥:  [{', '.join(f'{v.data:.4f}' for v in normed)}]")
rms_before = (sum(v.data**2 for v in test_vec) / len(test_vec))**0.5
rms_after = (sum(v.data**2 for v in normed) / len(normed))**0.5
print(f"    RMS ë³€í™˜: {rms_before:.4f} â†’ {rms_after:.4f} (â‰ˆ 1.0ì´ì–´ì•¼ í•¨)")
print(f"    ğŸ’¡ ê°’ì˜ í¬ê¸°ë¥¼ ì •ê·œí™”í•´ì„œ í•™ìŠµì´ ì•ˆì •ë˜ê²Œ í•œë‹¤")


# =============================================================================
# ì„¹ì…˜ 3: GPT ëª¨ë¸ í•¨ìˆ˜ (microgpt.py L107~143)
# =============================================================================
#
# [explain.md - Architecture ì„¹ì…˜ ìš”ì•½]
# gpt() í•¨ìˆ˜ëŠ” í•˜ë‚˜ì˜ í† í°(token_id)ì„ íŠ¹ì • ì‹œê°„ ìœ„ì¹˜(pos_id)ì—ì„œ ì²˜ë¦¬í•˜ê³ ,
# ì´ì „ ë°˜ë³µì˜ í™œì„±ê°’(keys, values = KV Cache)ì„ ì‚¬ìš©í•œë‹¤.
#
# ì²˜ë¦¬ ê³¼ì •:
# 1. Embeddings (ì„ë² ë”©):
#    - ì›ì‹œ í† í° IDë¥¼ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë²¡í„°(ìˆ«ì ë¦¬ìŠ¤íŠ¸)ë¡œ ë³€í™˜
#    - í† í° ì„ë² ë”©(wte) + ìœ„ì¹˜ ì„ë² ë”©(wpe) = í† í°ì´ ë¬´ì—‡ì´ê³  ì–´ë”” ìˆëŠ”ì§€ í‘œí˜„
#
# 2. Attention block (ì–´í…ì…˜ ë¸”ë¡):
#    - Query(Q): "ë‚´ê°€ ì°¾ëŠ” ê²ƒì€?"
#    - Key(K): "ë‚´ê°€ ê°€ì§„ ê²ƒì€?"
#    - Value(V): "ì„ íƒë˜ë©´ ë‚´ê°€ ì œê³µí•˜ëŠ” ê²ƒì€?"
#    - ì–´í…ì…˜ì€ í† í° tê°€ ê³¼ê±° ìœ„ì¹˜ 0..t-1ì„ "ë³´ëŠ”" ìœ ì¼í•œ ì¥ì†Œ
#    - ì–´í…ì…˜ = í† í° ê°„ í†µì‹  ë©”ì»¤ë‹ˆì¦˜
#
# 3. MLP block (MLP ë¸”ë¡):
#    - 2ì¸µ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬: 4ë°°ë¡œ í™•ì¥ â†’ ReLU â†’ ì›ë˜ë¡œ ì¶•ì†Œ
#    - ìœ„ì¹˜ë³„ ë…ë¦½ì ì¸ "ì‚¬ê³ " ìˆ˜í–‰
#    - íŠ¸ëœìŠ¤í¬ë¨¸ = í†µì‹ (Attention) + ê³„ì‚°(MLP) êµì°¨ ë°°ì¹˜
#
# 4. Residual connections (ì”ì°¨ ì—°ê²°):
#    - ì–´í…ì…˜/MLP ì¶œë ¥ì„ ìì‹ ì˜ ì…ë ¥ì— ë”í•¨ (x = a + b)
#    - ê·¸ë˜ë””ì–¸íŠ¸ê°€ ë„¤íŠ¸ì›Œí¬ë¥¼ ì§ì ‘ íë¥¼ ìˆ˜ ìˆê²Œ â†’ ê¹Šì€ ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥
#
# 5. Output (ì¶œë ¥):
#    - ìµœì¢… ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ì–´íœ˜ í¬ê¸°ë¡œ íˆ¬ì˜ (lm_head)
#    - 27ê°œ ìˆ«ì(ë¡œì§“) ì¶œë ¥. ë†’ì„ìˆ˜ë¡ ëª¨ë¸ì´ í•´ë‹¹ í† í°ì´ ë‹¤ìŒì— ì˜¬ ê²ƒì´ë¼ ìƒê°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# --- ì›ë³¸ ì½”ë“œ (microgpt.py L107~143) ---
def gpt(token_id, pos_id, keys, values):
    # â”€â”€â”€ 1. ì„ë² ë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tok_emb = state_dict['wte'][token_id]  # í† í° ì„ë² ë”© ë£©ì—…
    pos_emb = state_dict['wpe'][pos_id]    # ìœ„ì¹˜ ì„ë² ë”© ë£©ì—…
    x = [t + p for t, p in zip(tok_emb, pos_emb)]  # í† í° + ìœ„ì¹˜ ì„ë² ë”© í•©ì‚°
    x = rmsnorm(x)  # ì •ê·œí™” (ì”ì°¨ ì—°ê²° ë•Œë¬¸ì— í•„ìš”)

    for li in range(n_layer):
        # â”€â”€â”€ 2. Multi-head Attention ë¸”ë¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        x_residual = x
        x = rmsnorm(x)
        q = linear(x, state_dict[f'layer{li}.attn_wq'])  # ì¿¼ë¦¬
        k = linear(x, state_dict[f'layer{li}.attn_wk'])  # í‚¤
        v = linear(x, state_dict[f'layer{li}.attn_wv'])  # ê°’
        keys[li].append(k)
        values[li].append(v)
        x_attn = []
        for h in range(n_head):
            hs = h * head_dim
            q_h = q[hs:hs+head_dim]
            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]
            v_h = [vi[hs:hs+head_dim] for vi in values[li]]
            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]
            attn_weights = softmax(attn_logits)
            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]
            x_attn.extend(head_out)
        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])
        x = [a + b for a, b in zip(x, x_residual)]  # ì”ì°¨ ì—°ê²°

        # â”€â”€â”€ 3. MLP ë¸”ë¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        x_residual = x
        x = rmsnorm(x)
        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])  # 4ë°° í™•ì¥
        x = [xi.relu() for xi in x]                       # ë¹„ì„ í˜• í™œì„±í™”
        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])  # ì›ë˜ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
        x = [a + b for a, b in zip(x, x_residual)]  # ì”ì°¨ ì—°ê²°

    # â”€â”€â”€ 4. ì¶œë ¥ í”„ë¡œì ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logits = linear(x, state_dict['lm_head'])
    return logits


# =============================================================================
# ì„¹ì…˜ 4: "emma" ì²« í† í°(BOS)ì„ GPTì— ë„£ì–´ë³´ê¸° â€” í•œ ì¤„ì”© ì¶”ì 
# =============================================================================

print("\n\n" + "=" * 60)
print("ğŸ”¬ [ì„¹ì…˜ 3] 'emma' ì²˜ë¦¬ â€” BOS í† í°ë¶€í„° í•œ ì¤„ì”© ì¶”ì ")
print("=" * 60)

word = "emma"
tokens = [BOS] + [uchars.index(ch) for ch in word] + [BOS]
print(f"\n  '{word}' í† í°ì—´: {tokens}")
print(f"  â†’ [BOS, 'e', 'm', 'm', 'a', BOS] = [{', '.join(str(t) for t in tokens)}]")

# KV ìºì‹œ ì´ˆê¸°í™”
keys = [[] for _ in range(n_layer)]
values_cache = [[] for _ in range(n_layer)]

# â”€â”€â”€ ì²« ë²ˆì§¸ í† í° (BOS, pos=0) ì²˜ë¦¬ ê³¼ì •ì„ í•œ ì¤„ì”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
token_id, pos_id = tokens[0], 0
print(f"\n  === í† í° 0 ì²˜ë¦¬: token_id={token_id} (BOS), pos_id={pos_id} ===")

# Step 1: ì„ë² ë”© ë£©ì—…
tok_emb = state_dict['wte'][token_id]
pos_emb = state_dict['wpe'][pos_id]
print(f"\n  [Step 1] ì„ë² ë”© ë£©ì—…")
print(f"    tok_emb = wte[{token_id}] = [{', '.join(f'{v.data:+.4f}' for v in tok_emb)}]")
print(f"    pos_emb = wpe[{pos_id}]  = [{', '.join(f'{v.data:+.4f}' for v in pos_emb)}]")

x = [t + p for t, p in zip(tok_emb, pos_emb)]
print(f"    x = tok + pos    = [{', '.join(f'{v.data:+.4f}' for v in x)}]")

x = rmsnorm(x)
print(f"    x = rmsnorm(x)   = [{', '.join(f'{v.data:+.4f}' for v in x)}]")

# Step 2: Attention
print(f"\n  [Step 2] Multi-head Attention (layer 0)")
x_residual = x
x_norm = rmsnorm(x)
print(f"    x_norm = rmsnorm(x) = [{', '.join(f'{v.data:+.4f}' for v in x_norm)}]")

q = linear(x_norm, state_dict['layer0.attn_wq'])
k = linear(x_norm, state_dict['layer0.attn_wk'])
v = linear(x_norm, state_dict['layer0.attn_wv'])
print(f"    Q = linear(x, Wq) = [{', '.join(f'{v.data:+.4f}' for v in q)}]  ('{word}' í˜„ì¬ í† í°ì´ 'ì°¾ëŠ” ê²ƒ')")
print(f"    K = linear(x, Wk) = [{', '.join(f'{v.data:+.4f}' for v in k)}]  ('ê°€ì§„ ê²ƒ')")
print(f"    V = linear(x, Wv) = [{', '.join(f'{v.data:+.4f}' for v in v)}]  ('ì œê³µí•˜ëŠ” ê²ƒ')")

keys[0].append(k)
values_cache[0].append(v)

print(f"\n    [í—¤ë“œë³„ ì–´í…ì…˜ ê°€ì¤‘ì¹˜]")
x_attn = []
for h in range(n_head):
    hs = h * head_dim
    q_h = q[hs:hs+head_dim]
    k_h = [ki[hs:hs+head_dim] for ki in keys[0]]
    v_h = [vi[hs:hs+head_dim] for vi in values_cache[0]]
    attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]
    attn_weights = softmax(attn_logits)
    head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]
    x_attn.extend(head_out)

    print(f"    í—¤ë“œ {h}: Q_h=[{', '.join(f'{v.data:+.4f}' for v in q_h)}]")
    print(f"           attn_weights=[{', '.join(f'{w.data:.4f}' for w in attn_weights)}]")
    print(f"           â†’ í˜„ì¬ pos=0 (ì²« í† í°)ì´ë¼ ìê¸° ìì‹ ë§Œ ë³¼ ìˆ˜ ìˆìŒ â†’ ê°€ì¤‘ì¹˜=[1.0]")

x_attn_proj = linear(x_attn, state_dict['layer0.attn_wo'])
x = [a + b for a, b in zip(x_attn_proj, x_residual)]
print(f"\n    attn_out + residual = [{', '.join(f'{v.data:+.4f}' for v in x)}]")

# Step 3: MLP
print(f"\n  [Step 3] MLP ë¸”ë¡ (layer 0)")
x_residual = x
x_norm = rmsnorm(x)
x_up = linear(x_norm, state_dict['layer0.mlp_fc1'])
print(f"    mlp_fc1 ì¶œë ¥ (4ë°° í™•ì¥): ê¸¸ì´ {len(x_up)}, ì˜ˆ: [{', '.join(f'{v.data:+.4f}' for v in x_up[:4])}] ...")
x_relu = [xi.relu() for xi in x_up]
num_active = sum(1 for xi in x_relu if xi.data > 0)
print(f"    ReLU í›„: {num_active}/{len(x_relu)} ë‰´ëŸ° í™œì„± (ë‚˜ë¨¸ì§€ëŠ” 'ì£½ì€ ë‰´ëŸ°')")
x_down = linear(x_relu, state_dict['layer0.mlp_fc2'])
x = [a + b for a, b in zip(x_down, x_residual)]
print(f"    mlp + residual = [{', '.join(f'{v.data:+.4f}' for v in x)}]")

# Step 4: ì¶œë ¥
print(f"\n  [Step 4] ì¶œë ¥ í”„ë¡œì ì…˜ (lm_head)")
logits = linear(x, state_dict['lm_head'])
print(f"    logits (ê¸¸ì´ {len(logits)}): í† í° 0~4 â†’ [{', '.join(f'{v.data:+.4f}' for v in logits[:5])}] ...")

# logits â†’ í™•ë¥ 
probs = softmax(logits)

# ìƒìœ„ 5ê°œ í† í° ì¶œë ¥
prob_pairs = [(i, p.data) for i, p in enumerate(probs)]
prob_pairs.sort(key=lambda x: -x[1])
print(f"\n    [ëª¨ë¸ ì˜ˆì¸¡ â€” Top 5 í™•ë¥ ]")
for rank, (idx, prob) in enumerate(prob_pairs[:5]):
    token_str = "[BOS]" if idx == BOS else f"'{uchars[idx]}'"
    print(f"      #{rank+1}: {token_str:6s} (id={idx:2d}) â†’ {prob:.4f}")
print(f"    â†’ ì•„ì§ ëœë¤ íŒŒë¼ë¯¸í„°ë¼ì„œ ì˜ˆì¸¡ì´ ê· ë“±ì— ê°€ê¹Œì›€")


# =============================================================================
# ì„¹ì…˜ 5: "emma" ì „ì²´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ â€” ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê´€ì°°
# =============================================================================

print("\n\n" + "=" * 60)
print("ğŸ‘ï¸ [ì„¹ì…˜ 4] 'emma' ì „ì²´ ì‹œí€€ìŠ¤ â€” ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê´€ì°°")
print("=" * 60)

# ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ë©´ì„œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ìˆ˜ì§‘
keys2 = [[] for _ in range(n_layer)]
values2 = [[] for _ in range(n_layer)]
all_attn_weights = []  # ìœ„ì¹˜ë³„ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì €ì¥

# gpt í•¨ìˆ˜ë¥¼ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë¡í•˜ëŠ” ë²„ì „ìœ¼ë¡œ ìˆ˜ë™ ì‹¤í–‰
for pos_id in range(len(tokens) - 1):
    token_id = tokens[pos_id]
    token_str = "[BOS]" if token_id == BOS else f"'{uchars[token_id]}'"

    # ì„ë² ë”©
    tok_emb = state_dict['wte'][token_id]
    pos_emb = state_dict['wpe'][pos_id]
    x = [t + p for t, p in zip(tok_emb, pos_emb)]
    x = rmsnorm(x)

    # ì–´í…ì…˜
    x_residual = x
    x = rmsnorm(x)
    q = linear(x, state_dict['layer0.attn_wq'])
    k = linear(x, state_dict['layer0.attn_wk'])
    v = linear(x, state_dict['layer0.attn_wv'])
    keys2[0].append(k)
    values2[0].append(v)

    pos_attn = {}
    x_attn = []
    for h in range(n_head):
        hs = h * head_dim
        q_h = q[hs:hs+head_dim]
        k_h = [ki[hs:hs+head_dim] for ki in keys2[0]]
        v_h = [vi[hs:hs+head_dim] for vi in values2[0]]
        attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]
        attn_weights_h = softmax(attn_logits)
        head_out = [sum(attn_weights_h[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]
        x_attn.extend(head_out)
        pos_attn[h] = [w.data for w in attn_weights_h]

    all_attn_weights.append((pos_id, token_str, pos_attn))

    # MLP
    x = linear(x_attn, state_dict['layer0.attn_wo'])
    x = [a + b for a, b in zip(x, x_residual)]
    x_residual = x
    x = rmsnorm(x)
    x = linear(x, state_dict['layer0.mlp_fc1'])
    x = [xi.relu() for xi in x]
    x = linear(x, state_dict['layer0.mlp_fc2'])
    x = [a + b for a, b in zip(x, x_residual)]

# ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶œë ¥
token_labels = ["BOS", "e", "mâ‚", "mâ‚‚", "a"]

print(f"\n  [ì–´í…ì…˜ ê°€ì¤‘ì¹˜ â€” ê° ìœ„ì¹˜ê°€ ê³¼ê±° ì–´ëŠ í† í°ì— ì£¼ëª©í•˜ëŠ”ê°€?]")
print(f"  (ëœë¤ íŒŒë¼ë¯¸í„°ë¼ì„œ ì˜ë¯¸ ìˆëŠ” íŒ¨í„´ì€ í•™ìŠµ í›„ì— ë‚˜íƒ€ë‚¨)\n")

for pos_id, token_str, pos_attn in all_attn_weights:
    print(f"  ìœ„ì¹˜ {pos_id} ({token_str:5s}):")
    for h in range(n_head):
        weights = pos_attn[h]
        # ì‹œê°í™”: ë§‰ëŒ€ê·¸ë˜í”„
        bars = ""
        for t, w in enumerate(weights):
            bar_len = int(w * 20)
            bars += f"    {token_labels[t]:4s} {'â–ˆ' * bar_len}{'â–‘' * (20 - bar_len)} {w:.3f}\n"
        print(f"    í—¤ë“œ {h}:")
        print(bars, end="")


# =============================================================================
# í•µì‹¬ ì •ë¦¬
# =============================================================================
print("=" * 60)
print("ğŸ¯ í•µì‹¬ ì •ë¦¬")
print("=" * 60)
print(f"""
  1. íŒŒë¼ë¯¸í„°: ì‘ì€ ëœë¤ ìˆ«ìë¡œ ì´ˆê¸°í™”ëœ Valueë“¤ì˜ í–‰ë ¬ ì§‘í•©
     - wte ({vocab_size}Ã—{n_embd}): ê° í† í°ì˜ ë²¡í„° í‘œí˜„
     - wpe ({block_size}Ã—{n_embd}): ê° ìœ„ì¹˜ì˜ ë²¡í„° í‘œí˜„
     - attn_w* ({n_embd}Ã—{n_embd}): ì–´í…ì…˜ ë³€í™˜ í–‰ë ¬
     - mlp_fc1/fc2: í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
     - lm_head ({vocab_size}Ã—{n_embd}): ë²¡í„° â†’ ë¡œì§“ ë³€í™˜

  2. ëª¨ë¸ íë¦„:
     í† í°ID â†’ ì„ë² ë”© â†’ rmsnorm â†’ [ì–´í…ì…˜ â†’ ì”ì°¨] â†’ [MLP â†’ ì”ì°¨] â†’ lm_head â†’ ë¡œì§“

  3. ì–´í…ì…˜: í˜„ì¬ í† í°ì´ ê³¼ê±° í† í°ë“¤ì„ "ë³´ê³ " ì •ë³´ë¥¼ ëª¨ìœ¼ëŠ” í†µì‹  ë©”ì»¤ë‹ˆì¦˜
     - pos=0: ìê¸° ìì‹ ë§Œ ë³¼ ìˆ˜ ìˆìŒ (ê°€ì¤‘ì¹˜ = [1.0])
     - pos=n: 0~nê¹Œì§€ì˜ ëª¨ë“  ê³¼ê±° í† í°ì„ ë³¼ ìˆ˜ ìˆìŒ

  4. í˜„ì¬ëŠ” ëœë¤ íŒŒë¼ë¯¸í„°ë¼ì„œ ì˜ˆì¸¡ì´ ë¬´ì˜ë¯¸
     â†’ ë‹¤ìŒ ì±•í„°(í•™ìŠµ ë£¨í”„)ì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ë©´ ì˜ë¯¸ ìˆëŠ” ì˜ˆì¸¡ ì‹œì‘!

  ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {len(params)} (ì›ë³¸ microgpt: 4,192)
""")
